一、文本格式数据的读写：
    1.基础
        可以使用解析函数将文本数据转化成DataFrame
        Pandas的解析函数：
            read_csv -------------------------------------- 从文件、URL或文件型文件读取分隔好的数据，
                                                            逗号是默认分隔符
            read_table ------------------------------------ 从文件、URL或文件型文件读取分隔好的数据，
                                                            制表符('\t')是默认分隔符
            read_fwf -------------------------------------- 从特定宽度格式的文件中读取数据（无分隔符）
            read_clipboard -------------------------------- read_table的剪贴板版本，在将表格从Web页面上转换数据时有用
            read_excel ------------------------------------ 从Excel的XLS或XLSX文件中读取表格数据
            read_hdf -------------------------------------- 读取用pandas存储的HDF5文件
            read_html ------------------------------------- 从HTML文件中读取所有表格数据
            read_json ------------------------------------- 从json字符串中读取数据
            read_msgpack ---------------------------------- 读取MessagePack二进制格式的pandas数据
            read_pickle ----------------------------------- 读取一Python pickle格式存储的任意对象
            read_sas -------------------------------------- 读取存储在SAS系统中定制存储格式的SAS数据集
            read_sql -------------------------------------- 将SQL查询的结果（使用SQLAlchemy）读取为pandas的DataFrame
            read_stata ------------------------------------ 读取Stata格式的数据集
            read_feather ---------------------------------- 读取Feather二进制格式
        以上的函数有一些共有的可选参数：
            索引：                  可以将一个或者多个列作为返回的DataFrame，从文件或者用户处获得列名，或者没有列名
            类型推断和数据转化：     包括用户自定义的值转换和自定义的缺失值符号列表
            日期时间解析：           包括组合功能，也包括将分散在多个列上的日期和时间信息组合成结果中的单列
            迭代：                  支持对于大型文件的分块迭代
            未清洗数据的问题：       跳过行，页脚、注释以及其他次要数据，比如使用逗号分隔千位的数据
        有一部分的函数如read_csv会进行类型判断，因为列的数据类型并不是数据格式的一部分
        以pandas.read_csv/read_table为例：
            可见https://blog.csdn.net/weixin_44852067/article/details/122366383
            pd.read_csv/read_table(path,header,sep,names,index_col,skiprows,na_values)
                path:           表示打开的文件的地址字符串，URL或文件型对象
                header:         表示表头，默认文件的第一行就是表头，header=None或0表示没有表头(第一行作为表头)，header=int表示从第int+1行作为表头
                sep:            指定分隔符，可以自行设置分隔符，默认是','，read_tabel默认是\t，可以通过正则进行进行设置(Python引擎处理正则；C引擎处理特定的值，更快)
                delimiter:      类似sep
                index_col:      自己将某一列或者很多列（使用列表等）设置为index
                names:          表示自己的表头
                skiprows:       表示自己要跳过的行，可以跳过如注释等
                na_values:      自己设置的缺失值，在DataFrame中显示为Nan
                                可以通过设置一个value或者list表示global的缺失值，
                                也可以通过设置一个字典表示具体在某一行上的缺失值设置
                comment:        在行结尾处分隔注释的字符，只能是一个
                parse_datas:    尝试将数据解析为datetime，默认是False，
                                如果是True，将尝试解析所有的列，也可以指定某一（些）列进行解析，对于time默认为当前日期
                                如果是一个值，就是将对应的行进行解析
                                如果是一个列表，就是将对应的每个行分别进行解析
                                如果是一个列表的列表，就是将内部列表中的列作为一个整体进行解析
                                如果是一个字典，就是将key作为新的列的名字进行解析
                keep_date_col:  如果连接列到解析日期上，是否保留被连接的列，默认是Fasle
                converters:     一个字典key是列名,value是函数对象；表示在key列上apply value函数
                dayfirst:       解析非明确日期是，按照国际格式处理，默认是False
                date_parser:    用于解析日期的函数
                nrows:          从文件开头处读入的行数
                iterator:       是否返回一个TextParser对象，用于零散地读取文件，默认True
                chunksize:      用于迭代块的大小
                skip_footer:    忽略文件尾部的行数
                verbose:        打印各种解析器输出的信息，比如各种用时
                encodeing:      编码方式，默认Unicode
                squeeze:        如果解析数据只包含一列，返回Series
                thousands:      千位分隔符（,或.）
                
                    PS: 查看文件的一些方法，!type （windows操作系统下的路径）
                                          !cat (Linux操作系统下的路径)
    
    2.分块读入文本文件：
        当处理大型文件的时候可以通过小片段或者按小块遍历文件
            pd.options.display.max_rows --------------------------------- 设置显示的最大的行数
        如果只读取一小部分行，可以指明nrows,或skip_footer
        可以通过chunksize设置每一块的行数
        对于返回的TextParser迭代器对象，可以通过get_chunk(chunk_size)方法来确定读取的数据块，
            PS：需要在生成时chunksize=None
    
    3.将数据写入文本格式：
        前面提到了，可以使用read_csv函数读取csv对象中的数据
        下面来尝试将IPython中的数据写入csv文件
        对于DataFrame对象：
            DataFrameObj.to_csv(path,sep,na_rep,index,header,columns):
                path:       数据保存到的位置
                sep:        分隔符，默认是','，也可以自己设置
                na_rep:     将DataFrame中的Nan保存在csv中的符号，默认为空
                index:      是否将DataFrame中的index那一列也写入csv中
                header:     是否将DataFrame中的header那一行也写入csv中
                columns:    写入csv的 某个列 或者 多个列的列表
        对于Series对象：
            SeriesObj.to_csv-------- 方法参数同上
            可以通过以下的方法返回一个date_index对象
            pd.date_range(start,end,periods,freq,tz,normalize,name,inclusive,unit)  :
                start:      开启时间
                end:        结束时间
                periods:    index的长度（有几个）
                freq:       频率，包括Y，M，W-SUN/MON...，D，H，Min，S等
                tz:         时区，如'Asia/Hong_Kong','Europe/London'等
                normalize:  标准化
                name:       index的name
                inclusive:  是否包括两个边界，默认both
                unit:       单位
    
    4.设置自己的分隔形式：
        对于csv文件，可能会有各种不同的分隔符等设置，需要使用自定义方法进行读取
        可以自己设置一个类：
            class my_dialect(csv.Dialect):
                # 有以下的属性可以设置自己的类
                delimiter =         用于分隔的字符，默认是','
                lineterminator =    行终止符，默认是'\r\n'，读取器会自己识别并跨过
                quotechar =         用在含有特殊字符字段的引号，默认是' " '
                quoting =           应用惯例，包括csv.QUOTE_ALL,csv.QUOTE_MINIMAL等等
                skipinitialspace =  忽略每个分隔符后的空白，默认是False
                doublequote =       如何处理字段内部的引号，如果是True，则是双引号
                escapechar =        当引用设置csv.QUOTE_NONE时用于转义分隔符的字符串，默认禁止
    
    5.写入csv：
        writer = csv.writer(file,...)
        writer.writerow(row)
        writer.writerows(rows)
        略
    
    6.json数据：
        json库：
            load系列：json ----> Python变量
                json.loads(str) ---------------------------- 返回json字符串转换为的Python变量
                json.load(fp) ------------------------------ 返回json文件中的内容转换为的Python变量
            dump系列：Python变量 ----->json
                json.dumps(obj) ---------------------------- 返回将Python对象转化为的json字符串
                json.dump(obj,fp) -------------------------- 将Python对象写入对应的json文件
        pandas:
            read_json(fp) ---------------------------------- 将json中的内容直接转换成pandas对象
            DataFrameObj.to_json(fp,orient,date_format,double_precision,force_ascii,data_unit,default_unit,lines)
                参数：
                    fp:             保存路径，默认None->返回json字符串
                    orient:         指定json的key：
                                    split：     {index:[inidexs],columns:[columns],data:[values]}
                                    records:    {columns:{index:value}}
                                    index:      {index:{columns:value}}
                                    columns：   {columns:{index:value}}
                                    values:     值的json数组
                                    table:      数据库表格式
                    data_format:    字符串，日期转换类型
                    double_precision:浮点数小数位数
                    force_ascii:    强制转换为ascii
                    date_unit:      时间精度
                    default_handler:出现问题时处理的程序
                    lines:          如果是records，则每一行的每一条记录都写成json

    7.XML和HTML：网络抓取
        pandas有内建函数read_html可以使用lxml，bs4，html5lib等库将 html自动解析为DataFrame对象。
        pandas.read_html(path/fp/url) ------------------------- 遍历当前的html中的所有的table标签
                                                                并根据html中table组成的DataFrame的列表
        对于时间对象：
            pd.to_datetime(SeriesObj) ------------------------- 将SeriesObj的内容转化为datetime的Series
            datatimeSerObj.dt.year/month/days... -------------- 返回对应的year/month/days/...的Series对象
        
        XML读取：
            lxml的objectif模块可以解析XML文件
            objectify.parse(fp) ------------------------------- 将fp对象解析，返回lxml.etree._ElementTree对象
            ELT.getroot() ------------------------------------- 读取ELT中的root节点
            rootObj.getchildren() ----------------------------- 读取root上的各种子节点
            root.NOTENAME ------------------------------------- 生成一个NOTENAME节点的生成器
            noteObj.tag --------------------------------------- 标签名
            noteObj.get(attrbute) ----------------------------- 返回note中对应的属性，在XML中都是字符串
            noteObj.text -------------------------------------- 标签的文本
    
二、二进制格式：
    1.pickle类型：
        pickle是Python内建的序列化模块有实现存储数据（或序列化）最高效、最方便的方式之一
        pandas具有to_pickle方法将数据写入硬盘中。
            pdObj.to_pickle(path,compress) ------------------------------ pdObj的数据存储在path中，自定义压缩方法，也可以使用path后缀名指定压缩
            pd.read_pickle(path,*) -------------------------------------- 将path中的数据转化成Python对象，同上
    
    2.HDF5格式：
        HDF类似于一种字典，具有分级存储的功能，适用于长期存储数据
        pd.HDFStore(path,mode,complevel) -------------------------------- 生成一个pandas.io.pytables.HDFStore对象
                                                                          path表示打开hdf5文件的路径，
                                                                          mode表示打开方式，如'r'，'w'等
                                                                          complevel表示压缩的等级，0~9，0表示不压缩
        对于HDFStore对象：
            HDFStoreObj.put(key,value,format) --------------------------- 将value值存到key中（类似字典），
                                                                          format有'table'和'fixed'两种存储模式，前者更慢，但是支持特殊语法查询
            HDFStoreObj[key] = value ------------------------------------ 同上，但是不会设置格式
            HDFStoreObj[key] -------------------------------------------- 查询对应的key
            HDFStoreObj.select(key,where) ------------------------------- 从HDFStore中读取key中的数据，可以通过where进行筛选
                                                                          where可以是一个字符串的列表，使用Python语法，并进行类似bool数组的判断
            HDFStoreObj.close() ----------------------------------------- 关闭Store
        
        pandas中：
            pdObj.to_hdf(path,key,format) ------------------------------- 将pdOb中的Python数据直接存到path对应的文件
            pd.read_hdf(path,key,where) --------------------------------- 类似上面的select的方法，返回对应的pdObj
    
    3.Excel文件：
        读取Excel中的数据：
            pd.ExcelFile(path) ---------------------------- 生成一个ExcelFile对象，处理Excel，
                                                            可以使用不同的引擎，如openpyxl等
            pd.read_excel(path/EFObj,sheetname) ----------- 读取excel中的sheetname子表中的数据
            
            PS: ExcelFile对象可以认为和openpyxl中的WorkBook对象一致，具有相同的属性和使用方法
                openpyxl 的详细内容可以参见autopy有关的内容
        修改Excel中的数据：
            可以使用writer，方法和autopy中的方法类似
            可以将其中的 write 方法用 pdObj.to_excel(path/EFObj,sheetname) 方法替代
    
三、与Web API交互：
    可以简单地使用request模块，获取自己要获取地json数据
    resp.json() ------------------------------------- 将resp地json数据转换为Python字典变量
    有关request,bs4.selinum等模块的内容参考autopy

四、与数据库交互：
    
            